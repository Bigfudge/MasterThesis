2019-01-28: Started building indata for SVM. Prob. need align-script to decide
if word is wrong or not. Meeting with Dana, got feedback for planning report.
Should fix planning report next time.

2019-01-29: Hopefully fixed all feedback on planning report, can probably send
back to Dana tomorrow. Tomorrow should I finish the planning report and find and
use align-script (evaluation-script).

2019-01-30: Sent planning report to Dana and recived feedback. Got a mail from
Runeberg projekt about possible extended dataset. Need to figure out a way to
retrive and use it.

2019-01-31: Last fixes with planning report. Got the ocr-errors into the cvs-file aswell.
Tomorrow I need to add the Grepect also. And do everything again for tesseract. Also
need to download everything from Projekt Runeberg and find a way to use it.

2019-02-01: All words are hopefully in the cvs (except runeberg since I have decided
to wait with that). Calculates the frequency of a word in the corresponding "book"
(Argus/grepect).

2019-02-04: Planning report handed in. Added tri_grams frequency per word to the
.cvs file using a proper db. Need to redo all structure:
    -First build a db with all words and a column for each metric
    -Have a function for each metric and populates the db.

2019-02-05:
Added so that everything is saved in a db. If one metric needs to be recalculated
it can be done separately from the others. Need to get the SVM to start produce
result, maybe use free google cloud thing.

2019-02-06:
Started to train the svm. Might be good with some additional metric. Something
that depends on context? Got some result from the svm with 2000 random samples,
precision = 0.59, recall = 0.63, f1-score=0.52, support=400. Start looking at
error correction step:
MVP for correction stage:
    Finding Candidates
        Take all error-words and set editDistance=1
        Find all words in the corpus that have that editDistance. If no found
        then the editDistance is increased until a threashold.
    Selecting Candidates
        Select the word with the highest frequency in the corpus.

With sample_size=8000 and 'rbf' we got result  (p,r,f1,s)=(0.59,0.64,0.58,1600)
It takes ~38min to build the cvs-file.

2019-02-07:
Linear for 8000 samples (p,r,f1,s)= (0.73,0.67,0.56,1600). With Keras and a 8-8-1 conf.
got 93.29% accuracy. Started error correction program.

2019-02-08:
Hopefully completed error correction program and started putting all pieces
together. Need to be able to save the model so it does not train every time.
Also construct a main-program that puts everything together.

2019-02-11:
Generated first page of post-processed text. Unfourtunatly, had about 20%
lesser accuracy then without post-proccessing. Next step is to complete the
production and evaluation chain. First make it possible to postprocess a whole
directory then eval it with OCR Frontier.

2019-02-12:
Moved all path-strings to a constant file. Need to find a work around in order
to be able to post-process and evaluate at the same computer. Put the .txt files
from align.py in the correct folder. Construct a 5-gram context as a metric
for the input. This will probably take a long time to calculate. After that the
error_correction needs to be improved.

Error-detection inprovment:
- Construct a 5-gram context as a metric for the input.

Error-correction improvement:
    - Construct a weighted-character correction map.
    Possible approaches:
        - Extract letters from fraktur and calculate diff
        - "Borrow" the one keetunnen uses
    - Normalized longest common subsequence
    - Normalized maximal consecutive longest common subsequence
    - Put all three metrics into a weighted sum and decide candidate based on that.


If I could get {errousWord, correctWord}-pairs I can train a NN to correct it aswell.

2019-02-18:
    Started to generate pure OCR-output from Ocropus and Tesseract.
2019-02-19:
    Meeting with Dana. Tasks to do:
        Firstly create proper input via Ocropus and Tesseract.
        Rerun all training and evaluation.
        Normalize all input. Prune text of all tags and seperate words from signs(.,: etc).
        Rerun all training and evaluation.

    Idea that popped up:
        Make a program that can generate common OCR-errors and thus can create
        {correct_word, errorous_word}-pairs. Then we can train a NN to correct
        words.

        Need  to find a program that can rank common n-gram faults.
            r -> n
            nr->m
            ii->lu

        Need a program that randomly applies the faults(noice) to the correct words.

        Lastly a ML-program that learns to go from error_word-> correct_word
2019-02-20:
    Need to start over the generation of OCR-output since it needs to be seperated
    as argus and grepect.
2019-02-21:
    Finishing up the OCR-output generation. Removed all tags from ground truth.
    Started working with google cloud

2019-02-25:
    This week needs to be better. I need to finnish the OCR-programs once end for all and
    continue to improve the performance. This week its time to strart writing.

    Started up all the OCR-programs. If I can´t get right name of ocropus output,
    maybe ask Dana.

2019-02-26:
    Scraped ground-truth from runeberg.

2019-02-27:
    Finnally got all OCR-output and have now two working scripts for generating the images again.

2018-02-28:
	Evaluated the OCR-output (without Atikva).
    Result:
        TesseractArgus: WER: 36.85% CER: 65.01%
        TesseractGrepect: WER:33.74 CER: 62.93%
        OcropusArgus: WER:21.33% CER: 6.08%
        OcropusGrepect: WER:28.92% CER: -43.41%

    Rerun all training and evaluation on the new data with words and punctiation
    seperated (sample_size=8000, 10 pages from each source).
    Result:
        TesseractArgus: WER: 17.96% CER: 49.03%
        TesseractGrepect: WER: 15.17% CER: 50.72%
        OcropusArgus: WER: 13.20% CER:  -57.62%
        OcropusGrepect: WER: 15.76% CER:  -204.32%

    Todo tomorrow:
        Evaluate the initalSetup and updatedModel.
        Prepare for Seminar I
        Send email to Dana

    Todo next week:
        Remove all extra whitespace from input and output.
        Extend the word_freq list with extra words.
        Include the SB-evaluationscript into accruacyScript.py
        Start writing Half-time report

2018-03-01:
    Took 157min to compute all pictures. 250min compleatly from scratch.

    Rerun all training and evaluation on the new data with initial setup (sample_size=8000.
    Yielded bad result probably due to some encoding problem, look at CHARACC report.
    Result:
        TesseractArgus: WER: 30.47% CER: -1.51%
        TesseractGrepect: WER: 27.05% CER: -7.79%
        OcropusArgus: WER: 16.13% CER:  -92.47%
        OcropusGrepect: WER: 14.82% CER:  -210.89%

2018-03-04:
    Rerun all training and evaluation on the new data with initial setup (sample_size=8000.
    Fixed decoding problem.
    Result:
        TesseractArgus: WER: 36.74% CER: 65.76%%
        TesseractGrepect: WER: 33.56% CER: 61.23%
        OcropusArgus: WER: 21.30% CER:  -7.41%
        OcropusGrepect: WER: 28.90% CER:  -42.97%
    Rerun all training and evaluation on the new data with updated setup setup (sample_size=8000.
    Fixed decoding problem.
    Result:
        TesseractArgus: WER: 36.74% CER: 61.64%%
        TesseractGrepect: WER: 33.72% CER: 56.91%
        OcropusArgus: WER: 21.30% CER:  4.19%
        OcropusGrepect: WER: 28.90% CER:  -47.91%

    Todo this week:
        Look over CharAcc and WordAcc to see common problems.
            Remove all extra whitespace from input and output.
        Extend the word_freq list with extra words.
        Include the SB-evaluationscript into accruacyScript.py
        Start writing Half-time report

2018-03-05:
    Rerun evaluation with new ground_truth(without long s). OCR-output:
    Result:
        TesseractArgus: WER: 36.85% CER: 65.01%%
        TesseractGrepect: WER: 33.91% CER: 63.40%
        OcropusArgus: WER: 21.35% CER:  5.83%
        OcropusGrepect: WER: 29.04% CER:  -43.36%

    Rerun both initial_setup and dot_sep with new ground_truth:
    InititalSetup:
        SVM reultat: p=0.74      r=0.70      f1=0.62
        Resultat:
            TesseractArgus: WER: 36.74% CER: 65.76%%
            TesseractGrepect: WER: 33.73% CER: 61.69%
            OcropusArgus: WER: 21.30% CER:  7.41%
            OcropusGrepect: WER: 29.01% CER:  -42.21%

    Upgraded_Model_dot_sep:
        SVM reultat: p=0.73      r=0.70      f1=0.63
        Resultat:
            TesseractArgus: WER: 36.74% CER: 61.64%%
            TesseractGrepect: WER: 33.89% CER: 57.07%
            OcropusArgus: WER: 21.34% CER:  3.66%
            OcropusGrepect: WER: 38.44% CER:  -219.90%

    Should tokenize all words before using them instead of pruning for whitespace and so
    on.
    Todo tomorrow:
        Move the reports of Initial and upgraded to folder.
        complete the extention of accuracy-script
        Write.

2019-03-06:
    Running SB-evaluation. Reports moved.
    Idea:
        Implement so that all report automatically gets moved to Gdrive, or maybe
        e-mailed??
        Maybe set upp so when a folder called 'output' is uploaded the accuracy-script
        is executed and the reports is uploaded to another folder.

    The dot_sep is now main conf. i.e. in master.

    Increased samplesize to 30000 with standard conf:
    SVM reultat: p=0.78      r=0.80      f1=0.72

    Resultat:
        TesseractArgus: WER: 36.74% CER: 61.64%%
        TesseractGrepect: WER: 33.89% CER: 57.07%
        OcropusArgus: WER: 21.34% CER:  3.66%
        OcropusGrepect: WER: 38.44% CER:  -219.90%

    Increased samplesize to 100 000 with standard conf:
    SVM reultat: p=0.78      r=0.80      f1=0.72


2019-03-11:
    Todo:
        Evaluate tokenize output
        Finish noise_maker(All words should contain error. And two words -> one word)
        Prune ManuelTranscript
        Extend wordList
        Use noise_maker instead of align

    Resultat tokenize input:
        TesseractArgus: WER: 36.74% CER: 61.30%
        TesseractGrepect: WER: 33.89% CER: 57.09%
        OcropusArgus: WER: 21.34% CER:  3.67%
        OcropusGrepect: WER: 29.01% CER:  -42.90%

    Resultat pruned ManuelTranscript:
        OCR-output:
            TesseractArgus: WER: 36.85% CER: 64.92%
            TesseractGrepect: WER: 33.91% CER: 63.39%
            OcropusArgus: WER: 21.37% CER:  5.46%
            OcropusGrepect: WER: 29.04% CER:  -42.63%

        PP-output:
            TesseractArgus: WER: 36.74% CER: 61.27%
            TesseractGrepect: WER: 33.89% CER: 57.09%
            OcropusArgus: WER: 21.34% CER:  3.62%
            OcropusGrepect: WER: 29.01% CER:  -42.21.90%
2019-03-13:
    Todo:
        Move output to three different branches.
        Evaluate the different extentions of word-list.

        Dalin, 30 pages only:
            TesseractArgus: WER: 36.74% CER: 62.24%
            TesseractGrepect: WER: 33.89% CER: 56.48%
            OcropusArgus: WER: 21.72% CER:  -2.17%
            OcropusGrepect: WER: 29.01% CER:  -34.16%

2019-03-14:
        Dalin, runeberg, swedberg , 30 pages only:
            TesseractArgus: WER: 36.53% CER: 62.24%
            TesseractGrepect: WER: 33.96% CER: 56.48%
            OcropusArgus: WER: 21.72% CER:  -2.17%
            OcropusGrepect: WER: 28.72% CER:  -34.16%

        Todo:
            Complete partial evaluation of extended word-list
            Complete full evaluation of extended word-list

        I should possible change to the alternative word classifier since it have
        91.65% accuracy. With a larger network even ~95%, can it really be true?
        Idea:
            Set up NN that looks like word_classifiers but instead of 0/1 have the
            correct word. Maybe can correct, some words?

2019-03-15:
        Dalin:
            TesseractArgus: WER: 36.74% CER: 61.64%
            TesseractGrepect: WER: 33.89% CER: 57.07%
            OcropusArgus: WER: 21.34% CER:  3.66%
            OcropusGrepect: WER: 29.01% CER:  -47.20%

        Dalin, runeberg:
            TesseractArgus: WER: 36.74% CER: 61.64%
            TesseractGrepect: WER: 33.89% CER: 57.07%
            OcropusArgus: WER: 21.34% CER:  3.66%
            OcropusGrepect: WER: 29.01% CER:  -47.20%

        Dalin, runeberg, swedberg:
            TesseractArgus: WER: 36.74% CER: 61.64%
            TesseractGrepect: WER: 33.89% CER: 57.07%
            OcropusArgus: WER: 21.34% CER:  3.66%
            OcropusGrepect: WER: 29.01% CER:  -47.20%

        New word_classifier:
            TesseractArgus: WER: 36.53% CER: 62.24%
            TesseractGrepect: WER: 33.96% CER: 56.48%
            OcropusArgus: WER: 21.72% CER:  -2.17%
            OcropusGrepect: WER: 28.72% CER:  -34.16%

2019-03-16:
        Waiting for the new word_classifier to complete. Should plan on what to
        bring up at the meating on monday. Development stop until halftime report
        is handed in, it will focus my development. All ideas goes in log or
        backlog.
2019-03-18:
        Meeting with Dana. Writing for two weeks, after that i should focus the
        debugging on one text.
2019-03-19:
        Ideas:
            Should prune the OCR-output.
            Change get_non_alfanum to look at last character as well.
            Should fix how the erronous word is generated
            Should fix so swedishness is frequency-based and tri-gram is extracted correctly
            Test with higher edit_distant_threashold
            Add runeberg to word-list
            Add all n-grams as features? up to n=3

            !!SCALING IS VERY IMPORTANT IN SVM!!

            Follow the papers guide for SVM.
2019-03-27:
    Output, accually CER and WER not accuracy this time:
        TesseractArgus: WER: 63.15% CER: 34.99%
        TesseractGrepect: WER: 66.09% CER: 36.60%
        OcropusArgus: WER: 78.63% CER:  94.43%
        OcropusGrepect: WER: 70.96% CER:  142.62%

    Linear: 10 000:  0.73    0.69    0.61
            50 000:  0.70    0.67    0.58
            100 000:

    rbf:    10 000:  0.67      0.69      0.66
            50 000:  0.71      0.72      0.71
            100 000: 0.72      0.73      0.71
            Complete: 0.73      0.73      0.72

    poly:   10 000: took very long time, aborted after ~16h
            50 000:
            100 000:

    sigmoid:10 000:0.41      0.64      0.50
            50 000:0.55      0.62      0.55
            100 000:

2019-03-28:
    Things to discuss during meeting:
        Report
        Some of the things in the backlog maybe

2019-04-02:
    Finally handed in the half-time report. Now to programming!

    Added sample size option on gen_vector.

2019-03-03:
    After bug fixes 30 sampels, 8k SVM, 1500 words per work:
        TesseractArgus: WER: 79,79% CER: 41,37%
        TesseractGrepect: WER: 83,27% CER: 45,74%
        OcropusArgus: WER: 86,00% CER:  87,92%
        OcropusGrepect: WER: 83,72% CER:  118,55%

2019-04-04:
    rename 's/_ocropus_twomodel//g' *.txt
    wget -nd -r -l 1 -A txt  -e robots=off http://demo.spraakdata.gu.se/svedd/ocrproject/data/ocred/Argus_1732_pages/

2019-04-08:
    Number of words in word_freq without runeberg:
        1236883
        about seven times bigger with all of runeberg
    It becomes to slow with all those words.

    Todo:
        Conduct the grid_search for rbf
            Use np.arrange to further search for best parameters
        Construct a nice evaluationscript for PrimA

        rbf(C=1.1, gamma =1.4):    10 000:

                  precision    recall  f1-score   support

                  0       0.73      0.88      0.80      1311
                  1       0.63      0.39      0.48       689

        avg / total       0.70      0.71      0.69      2000

2019-04-09:
    Evaluation-script is now updated. Primary thing to improve is the word_classifier
    for valid words. Completed the split-word correction and changed start edit_dist=0

2019-04-17:
    Frontier:
    OCR-output:
        TesseractArgus: WER: 63.14% CER: 34.99%
        TesseractGrepect: WER: x% CER: 68.56%
        OcropusArgus: WER: 46.39% CER:  21.97%
        OcropusGrepect: WER: 37.60% CER:  17.59%

    PP-output:
        TesseractArgus: WER: 63.48% CER: 31.85%
        TesseractGrepect: WER: 68.57% CER: 36.93%
        OcropusArgus: WER: 49.16% CER:  23.49%
        OcropusGrepect: WER: 39.08% CER:  21.19%

2019-04-29:
    Evaluation for report (Baseline):
    Frontier:
        TesseractArgus: WER: 63.14% CER: 34.99%
        TesseractGrepect: WER: 67.63% CER: 68.56%
        OcropusArgus: WER: 46.39% CER:  21.97%
        OcropusGrepect: WER: 37.60% CER:  17.59%




2019-05-13:
        c_value=100 #1.1
        gamma=1000#1.3
        avg / total       0.63      0.62      0.62      1600

        c_value=10000 #1.1
        gamma=1000#1.3
        avg / total       0.64      0.64      0.63      1600


2019-05-14:
        Alla matrics förutom word gav:
        avg / total       0.82      0.71      0.68
        Bara baseline:
        avg / total       0.82      0.71      0.69
        Alla förutom  vowel och num_upper:
        avg / total       0.82      0.72      0.69
