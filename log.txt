2019-01-28: Started building indata for SVM. Prob. need align-script to decide
if word is wrong or not. Meeting with Dana, got feedback for planning report.
Should fix planning report next time.

2019-01-29: Hopefully fixed all feedback on planning report, can probably send
back to Dana tomorrow. Tomorrow should I finish the planning report and find and
use align-script (evaluation-script).

2019-01-30: Sent planning report to Dana and recived feedback. Got a mail from
Runeberg projekt about possible extended dataset. Need to figure out a way to
retrive and use it.

2019-01-31: Last fixes with planning report. Got the ocr-errors into the cvs-file aswell.
Tomorrow I need to add the Grepect also. And do everything again for tesseract. Also
need to download everything from Projekt Runeberg and find a way to use it.

2019-02-01: All words are hopefully in the cvs (except runeberg since I have decided
to wait with that). Calculates the frequency of a word in the corresponding "book"
(Argus/grepect).

2019-02-04: Planning report handed in. Added tri_grams frequency per word to the
.cvs file using a proper db. Need to redo all structure:
    -First build a db with all words and a column for each metric
    -Have a function for each metric and populates the db.

2019-02-05:
Added so that everything is saved in a db. If one metric needs to be recalculated
it can be done separately from the others. Need to get the SVM to start produce
result, maybe use free google cloud thing.

2019-02-06:
Started to train the svm. Might be good with some additional metric. Something
that depends on context? Got some result from the svm with 2000 random samples,
precision = 0.59, recall = 0.63, f1-score=0.52, support=400. Start looking at
error correction step:
MVP for correction stage:
    Finding Candidates
        Take all error-words and set editDistance=1
        Find all words in the corpus that have that editDistance. If no found
        then the editDistance is increased until a threashold.
    Selecting Candidates
        Select the word with the highest frequency in the corpus.

With sample_size=8000 and 'rbf' we got result  (p,r,f1,s)=(0.59,0.64,0.58,1600)
It takes ~38min to build the cvs-file.

2019-02-07:
Linear for 8000 samples (p,r,f1,s)= (0.73,0.67,0.56,1600). With Keras and a 8-8-1 conf.
got 93.29% accuracy. Started error correction program.

2019-02-08:
Hopefully completed error correction program and started putting all pieces
together. Need to be able to save the model so it does not train every time.
Also construct a main-program that puts everything together.

2019-02-11:
Generated first page of post-processed text. Unfourtunatly, had about 20%
lesser accuracy then without post-proccessing. Next step is to complete the
production and evaluation chain. First make it possible to postprocess a whole
directory then eval it with OCR Frontier.

2019-02-12:
Moved all path-strings to a constant file. Need to find a work around in order
to be able to post-process and evaluate at the same computer. Put the .txt files
from align.py in the correct folder. Construct a 5-gram context as a metric
for the input. This will probably take a long time to calculate. After that the
error_correction needs to be improved.

Error-detection inprovment:
- Construct a 5-gram context as a metric for the input.

Error-correction improvement:
    - Construct a weighted-character correction map.
    Possible approaches:
        - Extract letters from fraktur and calculate diff
        - "Borrow" the one keetunnen uses
    - Normalized longest common subsequence
    - Normalized maximal consecutive longest common subsequence
    - Put all three metrics into a weighted sum and decide candidate based on that.


If I could get {errousWord, correctWord}-pairs I can train a NN to correct it aswell.

2019-02-18:
    Started to generate pure OCR-output from Ocropus and Tesseract.
2019-02-19:
    Meeting with Dana. Tasks to do:
        Firstly create proper input via Ocropus and Tesseract.
        Rerun all training and evaluation.
        Normalize all input. Prune text of all tags and seperate words from signs(.,: etc).
        Rerun all training and evaluation.

    Idea that popped up:
        Make a program that can generate common OCR-errors and thus can create
        {correct_word, errorous_word}-pairs. Then we can train a NN to correct
        words.

        Need  to find a program that can rank common n-gram faults.
            r -> n
            nr->m
            ii->lu

        Need a program that randomly applies the faults(noice) to the correct words.

        Lastly a ML-program that learns to go from error_word-> correct_word
2019-02-20:
    Need to start over the generation of OCR-output since it needs to be seperated
    as argus and grepect.
2019-02-21:
    Finishing up the OCR-output generation. Removed all tags from ground truth.
    Started working with google cloud

2019-02-25:
    This week needs to be better. I need to finnish the OCR-programs once end for all and
    continue to improve the performance. This week its time to strart writing.

    Started up all the OCR-programs. If I canÂ´t get right name of ocropus output,
    maybe ask Dana.

2019-02-26:
    Scraped ground-truth from runeberg.

2019-02-27:
    Finnally got all OCR-output and have now two working scripts for generating the images again.

2018-02-28:
	Evaluated the OCR-output.    
