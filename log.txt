2019-01-28: Started building indata for SVM. Prob. need align-script to decide
if word is wrong or not. Meeting with Dana, got feedback for planning report.
Should fix planning report next time.

2019-01-29: Hopefully fixed all feedback on planning report, can probably send
back to Dana tomorrow. Tomorrow should I finish the planning report and find and
use align-script (evaluation-script).

2019-01-30: Sent planning report to Dana and recived feedback. Got a mail from
Runeberg projekt about possible extended dataset. Need to figure out a way to
retrive and use it.

2019-01-31: Last fixes with planning report. Got the ocr-errors into the cvs-file aswell.
Tomorrow I need to add the Grepect also. And do everything again for tesseract. Also
need to download everything from Projekt Runeberg and find a way to use it.

2019-02-01: All words are hopefully in the cvs (except runeberg since I have decided
to wait with that). Calculates the frequency of a word in the corresponding "book"
(Argus/grepect).

2019-02-04: Planning report handed in. Added tri_grams frequency per word to the
.cvs file using a proper db. Need to redo all structure:
    -First build a db with all words and a column for each metric
    -Have a function for each metric and populates the db.

2019-02-05:
Added so that everything is saved in a db. If one metric needs to be recalculated
it can be done separately from the others. Need to get the SVM to start produce
result, maybe use free google cloud thing.

2019-02-06:
Started to train the svm. Might be good with some additional metric. Something
that depends on context? Got some result from the svm with 2000 random samples,
precision = 0.59, recall = 0.63, f1-score=0.52, support=400. Start looking at
error correction step:
MVP for correction stage:
    Finding Candidates
        Take all error-words and set editDistance=1
        Find all words in the corpus that have that editDistance. If no found
        then the editDistance is increased until a threashold.
    Selecting Candidates
        Select the word with the highest frequency in the corpus.

With sample_size=8000 and 'rbf' we got result  (p,r,f1,s)=(0.59,0.64,0.58,1600)
It takes ~38min to build the cvs-file.

2019-02-07:
Linear for 8000 samples (p,r,f1,s)= (0.73,0.67,0.56,1600). With Keras and a 8-8-1 conf.
got 93.29% accuracy. Started error correction program.

2019-02-08:
Hopefully completed error correction program and started putting all pieces
together. Need to be able to save the model so it does not train every time.
Also construct a main-program that puts everything together.

2019-02-11:
Generated first page of post-processed text. Unfourtunatly, had about 20%
lesser accuracy then without post-proccessing. Next step is to complete the
production and evaluation chain. First make it possible to postprocess a whole
directory then eval it with OCR Frontier.

2019-02-12:
Moved all path-strings to a constant file. Need to find a work around in order
to be able to post-process and evaluate at the same computer. Put the .txt files
from align.py in the correct folder. Construct a 5-gram context as a metric
for the input. This will probably take a long time to calculate. After that the
error_correction needs to be improved.

Error-detection inprovment:
- Construct a 5-gram context as a metric for the input.

Error-correction improvement:
    - Construct a weighted-character correction map.
    Possible approaches:
        - Extract letters from fraktur and calculate diff
        - "Borrow" the one keetunnen uses
    - Normalized longest common subsequence
    - Normalized maximal consecutive longest common subsequence
    - Put all three metrics into a weighted sum and decide candidate based on that.


If I could get {errousWord, correctWord}-pairs I can train a NN to correct it aswell.

2019-02-18:
    Started to generate pure OCR-output from Ocropus and Tesseract.
2019-02-19:
    Meeting with Dana. Tasks to do:
        Firstly create proper input via Ocropus and Tesseract.
        Rerun all training and evaluation.
        Normalize all input. Prune text of all tags and seperate words from signs(.,: etc).
        Rerun all training and evaluation.

    Idea that popped up:
        Make a program that can generate common OCR-errors and thus can create
        {correct_word, errorous_word}-pairs. Then we can train a NN to correct
        words.

        Need  to find a program that can rank common n-gram faults.
            r -> n
            nr->m
            ii->lu

        Need a program that randomly applies the faults(noice) to the correct words.

        Lastly a ML-program that learns to go from error_word-> correct_word
2019-02-20:
    Need to start over the generation of OCR-output since it needs to be seperated
    as argus and grepect.
2019-02-21:
    Finishing up the OCR-output generation. Removed all tags from ground truth.
    Started working with google cloud

2019-02-25:
    This week needs to be better. I need to finnish the OCR-programs once end for all and
    continue to improve the performance. This week its time to strart writing.

    Started up all the OCR-programs. If I canÂ´t get right name of ocropus output,
    maybe ask Dana.

2019-02-26:
    Scraped ground-truth from runeberg.

2019-02-27:
    Finnally got all OCR-output and have now two working scripts for generating the images again.

2018-02-28:
	Evaluated the OCR-output (without Atikva).
    Result:
        TesseractArgus: WER: 36.85% CER: 65.01%
        TesseractGrepect: WER:33.74 CER: 62.93%
        OcropusArgus: WER:21.33% CER: 6.08%
        OcropusGrepect: WER:28.92% CER: -43.41%

    Rerun all training and evaluation on the new data with words and punctiation
    seperated (sample_size=8000, 10 pages from each source).
    Result:
        TesseractArgus: WER: 17.96% CER: 49.03%
        TesseractGrepect: WER: 15.17% CER: 50.72%
        OcropusArgus: WER: 13.20% CER:  -57.62%
        OcropusGrepect: WER: 15.76% CER:  -204.32%

    Todo tomorrow:
        Evaluate the initalSetup and updatedModel.
        Prepare for Seminar I
        Send email to Dana

    Todo next week:
        Remove all extra whitespace from input and output.
        Extend the word_freq list with extra words.
        Include the SB-evaluationscript into accruacyScript.py
        Start writing Half-time report

2018-03-01:
    Took 157min to compute all pictures. 250min compleatly from scratch.

    Rerun all training and evaluation on the new data with initial setup (sample_size=8000.
    Yielded bad result probably due to some encoding problem, look at CHARACC report.
    Result:
        TesseractArgus: WER: 30.47% CER: -1.51%
        TesseractGrepect: WER: 27.05% CER: -7.79%
        OcropusArgus: WER: 16.13% CER:  -92.47%
        OcropusGrepect: WER: 14.82% CER:  -210.89%

2018-03-04:
    Rerun all training and evaluation on the new data with initial setup (sample_size=8000.
    Fixed decoding problem.
    Result:
        TesseractArgus: WER: 36.74% CER: 65.76%%
        TesseractGrepect: WER: 33.56% CER: 61.23%
        OcropusArgus: WER: 21.30% CER:  -7.41%
        OcropusGrepect: WER: 28.90% CER:  -42.97%
    Rerun all training and evaluation on the new data with updated setup setup (sample_size=8000.
    Fixed decoding problem.
    Result:
        TesseractArgus: WER: 36.74% CER: 61.64%%
        TesseractGrepect: WER: 33.72% CER: 56.91%
        OcropusArgus: WER: 21.30% CER:  4.19%
        OcropusGrepect: WER: 28.90% CER:  -47.91%

    Todo this week:
        Look over CharAcc and WordAcc to see common problems.
            Remove all extra whitespace from input and output.
        Extend the word_freq list with extra words.
        Include the SB-evaluationscript into accruacyScript.py
        Start writing Half-time report

2018-03-05:
    Rerun evaluation with new ground_truth(without long s). OCR-output:
    Result:
        TesseractArgus: WER: 36.85% CER: 65.01%%
        TesseractGrepect: WER: 33.91% CER: 63.40%
        OcropusArgus: WER: 21.35% CER:  5.83%
        OcropusGrepect: WER: 29.04% CER:  -43.36%

    Rerun both initial_setup and dot_sep with new ground_truth:
    InititalSetup:
        SVM reultat: p=0.74      r=0.70      f1=0.62
        Resultat:
            TesseractArgus: WER: 36.74% CER: 65.76%%
            TesseractGrepect: WER: 33.73% CER: 61.69%
            OcropusArgus: WER: 21.30% CER:  7.41%
            OcropusGrepect: WER: 29.01% CER:  -42.21%

    Upgraded_Model_dot_sep:
        SVM reultat: p=0.73      r=0.70      f1=0.63
        Resultat:
            TesseractArgus: WER: 36.74% CER: 61.64%%
            TesseractGrepect: WER: 33.89% CER: 57.07%
            OcropusArgus: WER: 21.34% CER:  3.66%
            OcropusGrepect: WER: 38.44% CER:  -219.90%

    Should tokenize all words before using them instead of pruning for whitespace and so
    on.
    Todo tomorrow:
        Move the reports of Initial and upgraded to folder.
        complete the extention of accuracy-script
        Write.
